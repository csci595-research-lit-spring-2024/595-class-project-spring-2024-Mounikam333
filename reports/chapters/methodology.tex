\chapter{Methodology}
\label{ch:method}

\section{Algorithms description}
The algorithms essential for forest fire prediction, ranging from traditional regression to advanced ensemble methods, vital for understanding their roles in our study.
\begin{itemize}

\item Linear Regression

Linear regression is a simple and fast algorithm used for regression analysis. It models the relationship between a dependent variable and one or more independent variables by fitting a linear
equation to observed data points.

\item SVM Regressor

Support Vector Machine (SVM) regressor is a supervised learning algorithm used for regression tasks. It works by finding the hyperplane that best fits the data while maximizing the margin
between different classes.

\item Decision Tree Regressor

Decision tree regressor is a non-parametric supervised learning method used for regression tasks. It recursively splits the data into subsets based on the value of a chosen feature to predict the target variable.

\item Random Forest Regressor

Random Forest regressor is an ensemble learning method that constructs multiple decision trees during training and outputs the average prediction of the individual trees.

\item Extra Tree Regressor

Extra Tree regressor is another ensemble learning method similar to Random Forests but with slightly different tree construction methods.

\item XGBoost

XGBoost is a scalable and efficient gradient boosting library that is widely used for regression and classification tasks. It builds multiple decision trees iteratively and combines their predictions to improve accuracy.

\item LightGBM

LightGBM is a gradient boosting framework developed by Microsoft that focuses on leaf-wise tree growth and gradient-based learning. It is known for its high efficiency and performance.

\i em CatBoost
CatBoost is a gradient boosting library developed by Yandex that is designed to handle categorical features automatically. It is known for its robustness and ability to work with heterogeneous data
\end{itemize}


\section{Tables}
Data types of each column in the DataFrame df \\
\begin{table}[ht!]
    \centering
    \input{chapters/Table/Datatypes}
    \label{tab:Datatypes}
    \caption{Data types of each column in the DataFrame df}
\end{table}

\clearpage
\section{Code}
Code snippet in LATEX and this is a Python code example
\begin{lstlisting}
from ucimlrepo import fetch_ucirepo
import numpy np
import pandas pd
forest_fires = fetch_ucirepo(id=162)
X = forest_fires.data.features
Y = forest_fires.data.targets
df = pd.DataFrame(data=X, columns=forest_fires.feature_names)
df['target'] = Y
print(forest_fires.metadata)

print(forest_fires.variables)

print(df.head(10))

print("Statistical Description:", df.describe())

print("Data Types:", df.dtypes)

print("Correlation:", df.corr(method='pearson'))
import matplotlib.pyplot as plt
plt.figure(figsize=(6.5, 6.5))
df['target'].hist()
plt.title('Histogram of Target Column')
plt.xlabel('Target Values')
plt.ylabel('Frequency')
plt.show()
n_cols = len(df.columns)
layout = (n_cols // 2, 2)
plt.figure(figsize=(6.5, 6.5))
df.hist(layout=layout, figsize=(6.5, 6.5))
plt.tight_layout()
plt.show()
import numpy as np
fig, ax = plt.subplots(figsize=(6.5, 6.5))
cax = ax.matshow(df.corr(), vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0, len(df.columns), 1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(df.columns, rotation=45, ha='left')
ax.set_yticklabels(df.columns)
plt.show()
import matplotlib.pyplot as plt
plt.figure(figsize=(6.5, 6.5))
sns.barplot(x='month', y='target', data=df)
plt.title('Average Target by Month')
plt.xlabel('Month')
plt.ylabel('Average Target')
plt.show()
\end{lstlisting}

\clearpage
First 10 rows of Data frame
\begin{table}[ht!]
    \centering
    \input{chapters/Table/Dataframe}
    \label{tab:DataFrame}
    \caption{First 10 rows of Data frame}
\end{table}

\clearpage
\section{Figure}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=1.0]{figures/output_14_0.png}
    \caption{Histogram representation of Target column and Frequency.}
    \label{fig:example-01}
\end{figure}
\clearpage
Histograms for each column in the DataFrame using a specified layout
\begin{figure}[ht]
    \centering
    \includegraphics[scale=1.0]{figures/output_16_1.png}
    \caption{Histograms for each column in the DataFrame using a specified layout.}
    \label{fig:example-01}
\end{figure}

\clearpage
Bar plot showing the average target value for each month
\begin{figure}[ht]
    \centering
    \includegraphics[scale=1.0]{figures/output_18_0.png}
    \caption{Bar plot showing the average target value for each month.}
    \label{fig:example-01}
\end{figure}

\clearpage
\section {Implementation}
During our implementation phase, we strategically utilized XGBoost, known for its speed, accuracy,and advanced features such as regularization, parallelization, and feature importance scoring. 

This algorithm proved beneficial for balanced datasets incorporating both numerical and categorical features, as well as projects necessitating extensive documentation and community support.

LightGBM, renowned for its training speed, memory efficiency, and proficiency with large datasets, was a natural choice for scenarios with vast data volumes and concerns about overfitting. 

CatBoost, tailored for categorical features and imbalanced data, emerged as the preferred option for datasets characterized by categorical dominance and class imbalances, as well as projects seeking
efficient default settings and enhanced interpretability.


